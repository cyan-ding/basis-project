# BASIS Final Project

We do PGD (projected gradient descent) adversarial training to fine tune an instruct model to be more robust against attacks.

We then run various attack algorithms to benchmark the performance of our model.



