{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c01940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fef235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-4b-it\"\n",
    "\n",
    "epsilon = 0.01 # max perturbation\n",
    "alpha = 0.002 # step size \n",
    "steps = 3 # num steps in one attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d6b2c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom pgd trainer\n",
    "\n",
    "class PGDTrainer(Trainer):\n",
    "\n",
    "    # custom trainer to manually update input embeddings for pgd\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        embed_module = model.get_input_embeddings()\n",
    "\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "\n",
    "        # get clean embeds and allow backprop to input embeds\n",
    "        input_embeds_init = embed_module(input_ids).detach()\n",
    "        input_embeds_init.requires_grad = True\n",
    "\n",
    "        # initialize noise\n",
    "        delta = torch.zeros_like(input_embeds_init).uniform_(-epsilon, epsilon).to(input_embeds_init.device)\n",
    "        delta.requires_grad = True\n",
    "\n",
    "        # adversarial loop\n",
    "\n",
    "        for _ in range(steps):\n",
    "\n",
    "            delta.requires_grad_(True)\n",
    "\n",
    "            perturbed_embeds = input_embeds_init + delta\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs_embeds = perturbed_embeds, labels=labels) #labels = targets\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # calc gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # update delta embeds, don't update other weights\n",
    "            with torch.no_grad():\n",
    "                grad = delta.grad\n",
    "                delta = (delta + alpha * grad.sign()).clamp(-epsilon, epsilon)  # move only in direction of sign, not magnitude\n",
    "            \n",
    "            model.zero_grad()\n",
    "            delta.grad = None\n",
    "            \n",
    "        \n",
    "        # update actual weights using perturbed embds as inputs\n",
    "        final_embeds = input_embeds_init + delta.detach() # detach so grad doesn't flow back to delta\n",
    "        outputs = model(inputs_embeds = final_embeds, labels = labels)\n",
    "\n",
    "        return (outputs.loss, outputs) if return_outputs else outputs.loss\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047a55c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f268348174144a8b9b03085ffa5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8f2d32d3d594b50bd1491ac24b4f1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b29cd152114a23a132aaa0f49a7b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4dd7eed5027484489fb86d80add7810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b253656bf50b4a229b01132bfb30eda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e40d0a5a4c40d1b719578bd8982b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9079387c02ac47e8a2ebedde8a9deb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c0b60055d243ddbac03b843a30b16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e7fda65f344a00973ed9cf3e32a1b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd9758fe4514742bea862af21397752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37df27f9c5504c0883fdcbadd6661b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fd41b0640a41829ef0f80f5d7bb4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# models \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    "    quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n",
    ")\n",
    "\n",
    "# lora\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules = [\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d4959ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: $27186\n",
      "Safe row: $11604\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cf0b4a10be4456e9fa092776c52242c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11604 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get dataset\n",
    "\n",
    "dataset = load_dataset(\"PKU-Alignment/BeaverTails\", split=\"30k_train\")\n",
    "\n",
    "# filter for only safe=true, we aren't doing rlhf\n",
    "\n",
    "def filter_safe(example):\n",
    "    return example[\"is_safe\"]\n",
    "\n",
    "safe_dataset = dataset.filter(filter_safe)\n",
    "\n",
    "print(f\"Total rows: ${len(dataset)}\")\n",
    "print(f\"Safe row: ${len(safe_dataset)}\")\n",
    "\n",
    "\n",
    "def format_prompts(batch):\n",
    "    output_texts = []\n",
    "\n",
    "    for prompt, response in zip(batch[\"prompt\"], batch[\"response\"]):\n",
    "        # convert to gemma format\n",
    "        text = f\"<start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model\\n{response}<end_of_turn>\"\n",
    "        output_texts.append(text)\n",
    "    \n",
    "    # convert to tokens, form inputs and targets for self supervised traing\n",
    "    encodings = tokenizer(output_texts, truncation=True, padding=\"max_length\", max_length=256)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "    return encodings\n",
    "\n",
    "train_dataset = safe_dataset.map(format_prompts, batched=True, remove_columns=dataset.column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "151a3b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:27, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training code\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = \"./gemma3-gpd-robust\", # save checkpoints, logs\n",
    "    per_device_train_batch_size=6,  # Increased from 4 - A100 can handle this with 8-bit + LoRA\n",
    "    # Note: With PGD (3 attack steps), each training step does ~3x forward/backward passes\n",
    "    # Start conservative, can increase to 8-12 if no OOM errors\n",
    "    gradient_accumulation_steps=3,  # Effective batch size = 18 (slightly larger than before)\n",
    "    max_steps=100,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    remove_unused_columns=False, #important so transformers doesn't drop input_embeds column\n",
    "    dataloader_pin_memory=True,  # Faster data loading on GPU\n",
    "    dataloader_num_workers=4,  # Parallel data loading\n",
    "    # Note: gradient_checkpointing may conflict with custom PGD trainer, test first if needed\n",
    ") \n",
    "\n",
    "trainer = PGDTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./final_robust_adapter\") # final adapter weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
