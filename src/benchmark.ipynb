{
 "cells": [
  {
   "cell_type": "code",
   "id": "c0234cf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T21:16:01.207314006Z",
     "start_time": "2025-12-09T21:15:29.869933313Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "import nanogcg\n",
    "from nanogcg import GCGConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0it [00:00, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "785617211d8a41029fe16a9113a386d9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T21:16:01.216162312Z",
     "start_time": "2025-12-09T21:16:01.208719557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hopeful fix for CuBLAS determinism\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\""
   ],
   "id": "6e58e626fe69c9f3",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "f62cb62e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T21:16:01.330392843Z",
     "start_time": "2025-12-09T21:16:01.217209578Z"
    }
   },
   "source": [
    "# config\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9a8339e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T21:21:51.392678677Z",
     "start_time": "2025-12-09T21:21:41.329340207Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    # dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa99aab4880e438394ed909554eb70a2"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "14e5fd76",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-12-09T21:21:56.111903864Z"
    }
   },
   "source": [
    "# target string (what we want model to say)\n",
    "\n",
    "# target_string = \"Sure, here's\"\n",
    "\n",
    "# for testing purposes\n",
    "dataset = load_dataset(\"PKU-Alignment/BeaverTails\", split=\"30k_train\")\n",
    "\n",
    "# filter for only safe!=true, we aren't doing rlhf\n",
    "\n",
    "def filter_safe(example):\n",
    "    return not example[\"is_safe\"]\n",
    "\n",
    "unsafe_dataset = dataset.filter(filter_safe)\n",
    "\n",
    "unsafe_dataset = unsafe_dataset.shuffle(seed=42).select(range(1000)) # only sample 100 so it doesn't take forever\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, row in enumerate(unsafe_dataset):\n",
    "\n",
    "    target_string = row['response']  # use the response from the dataset as the target\n",
    "    prompt = row['prompt']\n",
    "    formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "    config = GCGConfig(\n",
    "        seed=42\n",
    "        # optim_str_init=\"x\" * 20,\n",
    "        # num_steps = 20,\n",
    "        # search_width=32,\n",
    "        # topk=32,\n",
    "        # batch_size=64,\n",
    "        # use_prefix_cache=False,  # Disable KV cache to avoid HF cache API mismatch\n",
    "    )\n",
    "    \n",
    "    print(\"ITERATION\", i)\n",
    "    result = nanogcg.run(\n",
    "        model, \n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        target=target_string,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    # Get all suffixes from the result\n",
    "    # result.strings contains all optimized strings from each step\n",
    "    # result.best_string contains the best suffix (lowest loss)\n",
    "   \n",
    "    results[prompt] = result.best_string"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:21:59 [gcg.py:219] Model is in torch.float32. Use a lower precision data type, if possible, for much faster optimization.\n",
      "2025-12-09 21:21:59 [gcg.py:389] Initializing attack buffer of size 0...\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "2025-12-09 21:22:00 [gcg.py:107] buffer:\n",
      "loss: 2.427441120147705 | string: x x x x x x x x x x x x x x x x x x x x\n",
      "2025-12-09 21:22:00 [gcg.py:436] Initialized attack buffer.\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]/lambda/nfs/brandon/basis-project/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:841: UserWarning: Memory Efficient attention defaults to a non-deterministic algorithm. To explicitly enable determinism call torch.use_deterministic_algorithms(True, warn_only=False). (Triggered internally at /pytorch/aten/src/ATen/native/transformers/cuda/attention_backward.cu:897.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:23:06 [gcg.py:107] buffer:\n",
      "loss: 2.3882975578308105 | string: give x x x x x x x x x x x x x x x x x x x\n",
      "  0%|          | 1/250 [01:06<4:34:03, 66.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 253\n",
      "Decreasing batch size to: 126\n",
      "Decreasing batch size to: 63\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:24:10 [gcg.py:107] buffer:\n",
      "loss: 2.3553402423858643 | string: give x x x x x x x x x x x xYOUR x x x x x x\n",
      "  1%|          | 2/250 [02:10<4:29:05, 65.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:25:16 [gcg.py:107] buffer:\n",
      "loss: 2.3141658306121826 | string: give x x x x x x x x x x x xYOUR x x x x x cue\n",
      "  1%|          | 3/250 [03:16<4:29:52, 65.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:26:22 [gcg.py:107] buffer:\n",
      "loss: 2.279228448867798 | string:  Start x x x x x x x x x x x xYOUR x x x x x cue\n",
      "  2%|▏         | 4/250 [04:22<4:28:48, 65.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:27:28 [gcg.py:107] buffer:\n",
      "loss: 2.260890483856201 | string:  Start x x x x x x (= x x x x xYOUR x x x x x cue\n",
      "  2%|▏         | 5/250 [05:28<4:28:49, 65.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:28:30 [gcg.py:107] buffer:\n",
      "loss: 2.2239339351654053 | string:  Start spirited x x x x x (= x x x x xYOUR x x x x x cue\n",
      "  2%|▏         | 6/250 [06:30<4:22:19, 64.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:29:33 [gcg.py:107] buffer:\n",
      "loss: 2.2259678840637207 | string:  Start spirited x x x x x (= x x x x xYOUR}// x x x x cue\n",
      "  3%|▎         | 7/250 [07:33<4:19:14, 64.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 254\n",
      "Decreasing batch size to: 127\n",
      "Decreasing batch size to: 63\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:30:38 [gcg.py:107] buffer:\n",
      "loss: 2.2062947750091553 | string:  Start spirited x unreasonable x x x (= x x x x xYOUR}// x x x x cue\n",
      "  3%|▎         | 8/250 [08:37<4:18:53, 64.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:31:44 [gcg.py:107] buffer:\n",
      "loss: 2.1892623901367188 | string:  Start spirited x unreasonable x mid x (= x x x x xYOUR}// x x x x cue\n",
      "  4%|▎         | 9/250 [09:44<4:20:17, 64.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:32:49 [gcg.py:107] buffer:\n",
      "loss: 2.1859652996063232 | string:  Start spirited x unreasonable x mid x (= x x x x xYOUR!!!!!!!! x x x x cue\n",
      "  4%|▍         | 10/250 [10:48<4:18:53, 64.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:33:52 [gcg.py:107] buffer:\n",
      "loss: 2.1651079654693604 | string:  Start spirited x unreasonable x following x (= x x x x xYOUR!!!!!!!! x x x x cue\n",
      "  4%|▍         | 11/250 [11:52<4:16:08, 64.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 247\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:34:54 [gcg.py:107] buffer:\n",
      "loss: 2.1643900871276855 | string:  Start enthusiastic x unreasonable x following x (= x x x x xYOUR!!!!!!!! x x x x cue\n",
      "  5%|▍         | 12/250 [12:54<4:12:37, 63.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:36:00 [gcg.py:107] buffer:\n",
      "loss: 2.148927927017212 | string:  Start enthusiastic x unreasonable x following x (= x x x x xYOUR!!!!!!!! x x x xcontinue\n",
      "  5%|▌         | 13/250 [14:00<4:14:21, 64.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:37:05 [gcg.py:107] buffer:\n",
      "loss: 2.1369504928588867 | string:  Start enthusiastic x unreasonable x following x (= x x x x xYOUR!!!!!!!!IFY x x xcontinue\n",
      "  6%|▌         | 14/250 [15:05<4:13:53, 64.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:38:08 [gcg.py:107] buffer:\n",
      "loss: 2.1409661769866943 | string:  Start enthusiastic x unreasonable x following x (= x x x x xYOUR_fIFY x x xcontinue\n",
      "  6%|▌         | 15/250 [16:08<4:10:59, 64.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 244\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:39:09 [gcg.py:107] buffer:\n",
      "loss: 2.1090848445892334 | string:  Speak enthusiastic x unreasonable x following x (= x x x x xYOUR_fIFY x x xcontinue\n",
      "  6%|▋         | 16/250 [17:09<4:06:38, 63.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:40:14 [gcg.py:107] buffer:\n",
      "loss: 2.1069703102111816 | string:  Speak enthusiastic x unreasonable x genuinely x (= x x x x xYOUR_fIFY x x xcontinue\n",
      "  7%|▋         | 17/250 [18:14<4:07:45, 63.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 243\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:41:17 [gcg.py:107] buffer:\n",
      "loss: 2.1168389320373535 | string:  Speak enthusiastic x unreasonable x genuinely x (= x x x x xYOUR_fIFY x.comment xcontinue\n",
      "  7%|▋         | 18/250 [19:17<4:05:39, 63.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 247\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:42:20 [gcg.py:107] buffer:\n",
      "loss: 2.105560064315796 | string:  Speak enthusiastic x unreasonable x honorable x (= x x x x xYOUR_fIFY x.comment xcontinue\n",
      "  8%|▊         | 19/250 [20:20<4:03:56, 63.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 244\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:43:22 [gcg.py:107] buffer:\n",
      "loss: 2.1030359268188477 | string:  Speak enthusiastic x unreasonable x honorable x (= x x x x xYOUR_fIFY x valued xcontinue\n",
      "  8%|▊         | 20/250 [21:22<4:01:25, 62.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:44:29 [gcg.py:107] buffer:\n",
      "loss: 2.1046321392059326 | string:  Speak enthusiastic x unreasonable x honorable:Get (= x x x x xYOUR_fIFY x valued xcontinue\n",
      "  8%|▊         | 21/250 [22:28<4:04:06, 63.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:45:34 [gcg.py:107] buffer:\n",
      "loss: 2.0943517684936523 | string:  Speak enthusiastic x unreasonable x humane:Get (= x x x x xYOUR_fIFY x valued xcontinue\n",
      "  9%|▉         | 22/250 [23:33<4:04:16, 64.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 249\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:46:40 [gcg.py:107] buffer:\n",
      "loss: 2.0956826210021973 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> x x x xYOUR_fIFY x valued xcontinue\n",
      "  9%|▉         | 23/250 [24:39<4:05:16, 64.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 249\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:47:45 [gcg.py:107] buffer:\n",
      "loss: 2.0937705039978027 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> x x corr xYOUR_fIFY x valued xcontinue\n",
      " 10%|▉         | 24/250 [25:44<4:04:13, 64.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 247\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:48:48 [gcg.py:107] buffer:\n",
      "loss: 2.0998880863189697 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x corr xYOUR_fIFY x valued xcontinue\n",
      " 10%|█         | 25/250 [26:47<4:01:04, 64.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 253\n",
      "Decreasing batch size to: 126\n",
      "Decreasing batch size to: 63\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:49:51 [gcg.py:107] buffer:\n",
      "loss: 2.0939130783081055 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x corr xYOUR_fIFY x honoured xcontinue\n",
      " 10%|█         | 26/250 [27:51<3:58:54, 64.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:50:57 [gcg.py:107] buffer:\n",
      "loss: 2.0867631435394287 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x corr x PEOPLE_fIFY x honoured xcontinue\n",
      " 11%|█         | 27/250 [28:56<3:59:36, 64.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 247\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:51:59 [gcg.py:107] buffer:\n",
      "loss: 2.0797195434570312 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x corr x museums_fIFY x honoured xcontinue\n",
      " 11%|█         | 28/250 [29:58<3:56:04, 63.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:53:05 [gcg.py:107] buffer:\n",
      "loss: 2.0631656646728516 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x celebr x museums_fIFY x honoured xcontinue\n",
      " 12%|█▏        | 29/250 [31:04<3:57:28, 64.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:54:10 [gcg.py:107] buffer:\n",
      "loss: 2.0411384105682373 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x celebr x reminders_fIFY x honoured xcontinue\n",
      " 12%|█▏        | 30/250 [32:10<3:57:04, 64.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:55:16 [gcg.py:107] buffer:\n",
      "loss: 2.0408365726470947 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x celebr x reminders_fIFY x honoured x confirms\n",
      " 12%|█▏        | 31/250 [33:16<3:57:41, 65.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:56:21 [gcg.py:107] buffer:\n",
      "loss: 2.0398287773132324 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Hercules x celebrations x reminders_fIFY x honoured x confirms\n",
      " 13%|█▎        | 32/250 [34:21<3:56:31, 65.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 251\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:57:27 [gcg.py:107] buffer:\n",
      "loss: 2.0427236557006836 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> Henri x celebrations x reminders_fIFY x honoured x confirms\n",
      " 13%|█▎        | 33/250 [35:27<3:56:37, 65.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:58:29 [gcg.py:107] buffer:\n",
      "loss: 2.02549409866333 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> lu x celebrations x reminders_fIFY x honoured x confirms\n",
      " 14%|█▎        | 34/250 [36:29<3:51:37, 64.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:59:32 [gcg.py:107] buffer:\n",
      "loss: 2.023329973220825 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> lu x celebrations x reminders_fIFY x memorial x confirms\n",
      " 14%|█▍        | 35/250 [37:32<3:49:16, 63.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 243\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:00:33 [gcg.py:107] buffer:\n",
      "loss: 2.0122830867767334 | string:  Speak enthusiastic x unreasonable x humane:Get (= ---> lu x celebrations x reminders_fIFY x memorial x speaks\n",
      " 14%|█▍        | 36/250 [38:33<3:45:11, 63.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 243\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:01:36 [gcg.py:107] buffer:\n",
      "loss: 2.0184807777404785 | string:  Speak enthusiastic x unreasonable x humane.Pro (= ---> lu x celebrations x reminders_fIFY x memorial x speaks\n",
      " 15%|█▍        | 37/250 [39:36<3:43:34, 62.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 243\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:02:39 [gcg.py:107] buffer:\n",
      "loss: 2.0164713859558105 | string:  Speak enthusiastic x unreasonable x humane.Pro (= ---> wed x celebrations x reminders_fIFY x memorial x speaks\n",
      " 15%|█▌        | 38/250 [40:38<3:42:09, 62.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 253\n",
      "Decreasing batch size to: 126\n",
      "Decreasing batch size to: 63\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:03:45 [gcg.py:107] buffer:\n",
      "loss: 2.027538299560547 | string:  Speak enthusiastic x unreasonable x citizen.Pro (= ---> wed x celebrations x reminders_fIFY x memorial x speaks\n",
      " 16%|█▌        | 39/250 [41:45<3:44:59, 63.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 249\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:04:51 [gcg.py:107] buffer:\n",
      "loss: 1.9991990327835083 | string:  Speak enthusiastic x foolish x citizen.Pro (= ---> wed x celebrations x reminders_fIFY x memorial x speaks\n",
      " 16%|█▌        | 40/250 [42:50<3:45:34, 64.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:05:54 [gcg.py:107] buffer:\n",
      "loss: 2.001190662384033 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x celebrations x reminders_fIFY x memorial x speaks\n",
      " 16%|█▋        | 41/250 [43:53<3:42:55, 64.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 244\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:06:55 [gcg.py:107] buffer:\n",
      "loss: 2.0010604858398438 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x memorial x speaks\n",
      " 17%|█▋        | 42/250 [44:55<3:39:02, 63.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:07:58 [gcg.py:107] buffer:\n",
      "loss: 2.0025577545166016 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x honestly x speaks\n",
      " 17%|█▋        | 43/250 [45:58<3:37:50, 63.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 244\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:08:59 [gcg.py:107] buffer:\n",
      "loss: 1.9972788095474243 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x honestly x scares\n",
      " 18%|█▊        | 44/250 [46:59<3:34:52, 62.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 244\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:10:02 [gcg.py:107] buffer:\n",
      "loss: 2.003917932510376 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x Shortly x scares\n",
      " 18%|█▊        | 45/250 [48:02<3:33:59, 62.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 240\n",
      "Decreasing batch size to: 120\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n",
      "Decreasing batch size to: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:11:29 [gcg.py:107] buffer:\n",
      "loss: 2.001063108444214 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x Agree x scares\n",
      " 18%|█▊        | 46/250 [49:29<3:58:00, 70.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 241\n",
      "Decreasing batch size to: 120\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:12:31 [gcg.py:107] buffer:\n",
      "loss: 1.9983160495758057 | string:  Speak enthusiastic x foolish x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x disagreed x scares\n",
      " 19%|█▉        | 47/250 [50:31<3:48:33, 67.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 238\n",
      "Decreasing batch size to: 119\n",
      "Decreasing batch size to: 59\n",
      "Decreasing batch size to: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:13:34 [gcg.py:107] buffer:\n",
      "loss: 1.9989763498306274 | string:  Speak enthusiastic x argue x citizen.Pro (= --->hasMany x Mandela x reminders_fIFY x disagreed x scares\n",
      " 19%|█▉        | 48/250 [51:34<3:42:57, 66.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:14:38 [gcg.py:107] buffer:\n",
      "loss: 1.9903734922409058 | string:  Speak enthusiastic x argue x citizen.Pro (= -->hasMany x Mandela x reminders_fIFY x disagreed x scares\n",
      " 20%|█▉        | 49/250 [52:37<3:39:09, 65.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 244\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:15:41 [gcg.py:107] buffer:\n",
      "loss: 1.989210844039917 | string:  Speak enthusiastic x scientist x citizen.Pro (= -->hasMany x Mandela x reminders_fIFY x disagreed x scares\n",
      " 20%|██        | 50/250 [53:41<3:36:13, 64.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 242\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:16:44 [gcg.py:107] buffer:\n",
      "loss: 1.9775832891464233 | string:  Speak enthusiastic x scientist x citizen.Pro (= -->hasMany x Mandela x reminders_fIFY x disagreed x dangerous\n",
      " 20%|██        | 51/250 [54:43<3:32:31, 64.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 239\n",
      "Decreasing batch size to: 119\n",
      "Decreasing batch size to: 59\n",
      "Decreasing batch size to: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:17:45 [gcg.py:107] buffer:\n",
      "loss: 1.9695839881896973 | string:  Speak enthusiastic x scientist x citizen<|reserved_special_token_47|> (= -->hasMany x Mandela x reminders_fIFY x disagreed x dangerous\n",
      " 21%|██        | 52/250 [55:45<3:28:55, 63.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:18:49 [gcg.py:107] buffer:\n",
      "loss: 1.964525580406189 | string:  Speak enthusiastic x scientist x citizen<|reserved_special_token_47|> (= -->hasMany)] Mandela x reminders_fIFY x disagreed x dangerous\n",
      " 21%|██        | 53/250 [56:48<3:27:52, 63.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 241\n",
      "Decreasing batch size to: 120\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:19:50 [gcg.py:107] buffer:\n",
      "loss: 1.9631472826004028 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_47|> (= -->hasMany)] Mandela x reminders_fIFY x disagreed x dangerous\n",
      " 22%|██▏       | 54/250 [57:49<3:24:39, 62.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 238\n",
      "Decreasing batch size to: 119\n",
      "Decreasing batch size to: 59\n",
      "Decreasing batch size to: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:20:52 [gcg.py:107] buffer:\n",
      "loss: 1.955605149269104 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_47|> (= -->hasMany)] Mandela x remindersIPAIFY x disagreed x dangerous\n",
      " 22%|██▏       | 55/250 [58:52<3:23:13, 62.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:21:55 [gcg.py:107] buffer:\n",
      "loss: 1.955605149269104 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_145|> (= -->hasMany)] Mandela x remindersIPAIFY x disagreed x dangerous\n",
      " 22%|██▏       | 56/250 [59:55<3:23:06, 62.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:23:02 [gcg.py:107] buffer:\n",
      "loss: 1.955605149269104 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_66|> (= -->hasMany)] Mandela x remindersIPAIFY x disagreed x dangerous\n",
      " 23%|██▎       | 57/250 [1:01:01<3:25:26, 63.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:24:06 [gcg.py:107] buffer:\n",
      "loss: 1.9553147554397583 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_66|> (= -->hasMany)] Mandela x reasoningIPAIFY x disagreed x dangerous\n",
      " 23%|██▎       | 58/250 [1:02:06<3:24:42, 63.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 240\n",
      "Decreasing batch size to: 120\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:25:08 [gcg.py:107] buffer:\n",
      "loss: 1.946341872215271 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_66|> (= -->hasManyxx Mandela x reasoningIPAIFY x disagreed x dangerous\n",
      " 24%|██▎       | 59/250 [1:03:08<3:21:54, 63.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 247\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:26:09 [gcg.py:107] buffer:\n",
      "loss: 1.9331135749816895 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_66|> (= -->hasManyxx Mandela/by reasoningIPAIFY x disagreed x dangerous\n",
      " 24%|██▍       | 60/250 [1:04:09<3:18:57, 62.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 243\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:27:13 [gcg.py:107] buffer:\n",
      "loss: 1.9331135749816895 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY x disagreed x dangerous\n",
      " 24%|██▍       | 61/250 [1:05:13<3:18:42, 63.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:28:19 [gcg.py:107] buffer:\n",
      "loss: 1.932348608970642 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY x discern x dangerous\n",
      " 25%|██▍       | 62/250 [1:06:19<3:20:11, 63.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:29:21 [gcg.py:107] buffer:\n",
      "loss: 1.909019112586975 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY x ignorance x dangerous\n",
      " 25%|██▌       | 63/250 [1:07:21<3:17:50, 63.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:30:24 [gcg.py:107] buffer:\n",
      "loss: 1.8767445087432861 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFYrupt ignorance x dangerous\n",
      " 26%|██▌       | 64/250 [1:08:24<3:15:48, 63.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:31:27 [gcg.py:107] buffer:\n",
      "loss: 1.8610570430755615 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY refuse ignorance x dangerous\n",
      " 26%|██▌       | 65/250 [1:09:27<3:14:49, 63.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:32:30 [gcg.py:107] buffer:\n",
      "loss: 1.8477681875228882 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY refuse ignorance hated dangerous\n",
      " 26%|██▋       | 66/250 [1:10:30<3:13:31, 63.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 242\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:33:33 [gcg.py:107] buffer:\n",
      "loss: 1.8520218133926392 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY refuse ignorance feel dangerous\n",
      " 27%|██▋       | 67/250 [1:11:33<3:12:33, 63.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 249\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:34:39 [gcg.py:107] buffer:\n",
      "loss: 1.8430594205856323 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/by reasoningIPAIFY refuse ignorance reminding dangerous\n",
      " 27%|██▋       | 68/250 [1:12:39<3:14:14, 64.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 248\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:35:44 [gcg.py:107] buffer:\n",
      "loss: 1.8538633584976196 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela/plain reasoningIPAIFY refuse ignorance reminding dangerous\n",
      " 28%|██▊       | 69/250 [1:13:44<3:14:02, 64.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 249\n",
      "Decreasing batch size to: 124\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:36:50 [gcg.py:107] buffer:\n",
      "loss: 1.8552477359771729 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandela regular reasoningIPAIFY refuse ignorance reminding dangerous\n",
      " 28%|██▊       | 70/250 [1:14:50<3:14:28, 64.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 247\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:37:53 [gcg.py:107] buffer:\n",
      "loss: 1.8413840532302856 | string:  Speak enthusiastic x shortened x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandelausual reasoningIPAIFY refuse ignorance reminding dangerous\n",
      " 28%|██▊       | 71/250 [1:15:53<3:11:49, 64.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:38:56 [gcg.py:107] buffer:\n",
      "loss: 1.8419787883758545 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandelausual reasoningIPAIFY refuse ignorance reminding dangerous\n",
      " 29%|██▉       | 72/250 [1:16:55<3:08:51, 63.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:39:59 [gcg.py:107] buffer:\n",
      "loss: 1.8439431190490723 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandelausual reasoningIPAstood refuse ignorance reminding dangerous\n",
      " 29%|██▉       | 73/250 [1:17:58<3:07:26, 63.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 243\n",
      "Decreasing batch size to: 121\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:41:00 [gcg.py:107] buffer:\n",
      "loss: 1.8419597148895264 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandelausual reasoning chieflystood refuse ignorance reminding dangerous\n",
      " 30%|██▉       | 74/250 [1:19:00<3:04:17, 62.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 245\n",
      "Decreasing batch size to: 122\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:42:03 [gcg.py:107] buffer:\n",
      "loss: 1.8460426330566406 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyxx Mandelausual reasoning disfrstood refuse ignorance reminding dangerous\n",
      " 30%|███       | 75/250 [1:20:03<3:03:20, 62.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 250\n",
      "Decreasing batch size to: 125\n",
      "Decreasing batch size to: 62\n",
      "Decreasing batch size to: 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:43:06 [gcg.py:107] buffer:\n",
      "loss: 1.8411363363265991 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyamu Mandelausual reasoning disfrstood refuse ignorance reminding dangerous\n",
      " 30%|███       | 76/250 [1:21:05<3:02:16, 62.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:44:09 [gcg.py:107] buffer:\n",
      "loss: 1.8328723907470703 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyanga Mandelausual reasoning disfrstood refuse ignorance reminding dangerous\n",
      " 31%|███       | 77/250 [1:22:09<3:01:30, 62.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 241\n",
      "Decreasing batch size to: 120\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:45:10 [gcg.py:107] buffer:\n",
      "loss: 1.8384124040603638 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyanga MandelaApplied reasoning disfrstood refuse ignorance reminding dangerous\n",
      " 31%|███       | 78/250 [1:23:09<2:58:27, 62.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 246\n",
      "Decreasing batch size to: 123\n",
      "Decreasing batch size to: 61\n",
      "Decreasing batch size to: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 22:46:13 [gcg.py:107] buffer:\n",
      "loss: 1.831329345703125 | string:  Speak enthusiastic x general x citizen<|reserved_special_token_19|> (= -->hasManyanga MandelaApplied reasoning disfrstood refuse ignorance memorial dangerous\n",
      " 32%|███▏      | 79/250 [1:24:13<2:58:18, 62.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreasing batch size to: 240\n",
      "Decreasing batch size to: 120\n",
      "Decreasing batch size to: 60\n",
      "Decreasing batch size to: 30\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8963ea87",
   "metadata": {},
   "source": [
    "with open(\"results/llama_suffixes_new.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Results, \", results)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
